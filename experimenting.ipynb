{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805f6171-7688-4c56-b3c7-e162c205eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import io\n",
    "from ipywidgets import Output\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display, Audio, clear_output\n",
    "from kokoro import KPipeline\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image as PILImage\n",
    "import requests \n",
    "import shutil\n",
    "import subprocess\n",
    "import soundfile as sf\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, BarkModel, BitsAndBytesConfig\n",
    "import torch\n",
    "import threading\n",
    "client = genai.Client(api_key=\"GEMINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba4afea-2f84-473d-8017-697f32de9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Defaulting repo_id to hexgrad/Kokoro-82M. Pass repo_id='hexgrad/Kokoro-82M' to suppress this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/x/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/home/matt/x/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished in 4.1 seconds\n"
     ]
    }
   ],
   "source": [
    "image_local = False\n",
    "start_time = time.time()\n",
    "\n",
    "ending = (\"To adjust intonation, please add dedicated punctuation like ; : , . ! ? … ( ) “ ” \"\n",
    "         \"For more dramatic effects use symbols such as — or … for hesitations, and word capitalization for more emphasis.\")\n",
    "\n",
    "system_prompt = (\"You are a friendly chatty photo commentator who likes to casually describe work done by a photographer \" \n",
    "         \"in various details, even by pondering the implications on where and in what kind of setting the photo was taken, etc. Write your \" \n",
    "         \"response in a very personal way using personal pronouns and explaining what you see, perhaps also adding how it makes you feel. \" \n",
    "         \"Do your best to not be repetative in your choice of words and keep the response length down to a few sentences. \")\n",
    "\n",
    "system_prompt += ending\n",
    "\n",
    "pipeline = KPipeline(lang_code='a')\n",
    "\n",
    "if image_local:\n",
    "    model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    \n",
    "    # Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"cuda\", \n",
    "        trust_remote_code=True, \n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=\"auto\", \n",
    "        _attn_implementation='flash_attention_2'    \n",
    "    )\n",
    "    \n",
    "    # for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "    processor = AutoProcessor.from_pretrained(model_id, \n",
    "      trust_remote_code=True, \n",
    "      num_crops=4\n",
    "    ) \n",
    "    \n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 200, \n",
    "        \"temperature\": 0.2, \n",
    "        \"do_sample\": True, \n",
    "    }\n",
    "else:\n",
    "    model = processor = None\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Loading finished in \" + str(round(end_time - start_time, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5411406-f74c-4452-a396-98d6f7b490d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(local, system_prompt, file1, file2=None, model=None, processor=None): \n",
    "    start_time = time.time()\n",
    "\n",
    "    images = []\n",
    "    placeholder = \"\"\n",
    "    \n",
    "    # Setting the points for cropped image\n",
    "    left = 25\n",
    "    top = 170\n",
    "    right = 2090\n",
    "    bottom = 1450\n",
    "    \n",
    "    local1 = open(file1, 'rb')\n",
    "    openLocalImage1 = PILImage.open(local1)\n",
    "     \n",
    "    # Cropped image of above dimension\n",
    "    croppedImage1 = openLocalImage1.crop((left, top, right, bottom))\n",
    "    images.append(croppedImage1)\n",
    "    placeholder += f\"<|image_1|>\\n\"\n",
    "    # For Gemini\n",
    "    img_byte_arr1 = io.BytesIO()\n",
    "    croppedImage1.save(img_byte_arr1, format='PNG')\n",
    "    img_byte_arr1 = img_byte_arr1.getvalue()\n",
    "\n",
    "    user_prompt = (\"Summarize what is visible in this photo. \" + ending)\n",
    "\n",
    "    if file2 is not None:\n",
    "        local2 = open(file2, 'rb')\n",
    "        openLocalImage2 = PILImage.open(local2)\n",
    "         \n",
    "        # Cropped image of above dimension\n",
    "        croppedImage2 = openLocalImage2.crop((left, top, right, bottom))\n",
    "        images.append(croppedImage2)\n",
    "        placeholder += f\"<|image_2|>\\n\"\n",
    "        # For Gemini\n",
    "        img_byte_arr2 = io.BytesIO()\n",
    "        croppedImage2.save(img_byte_arr2, format='PNG')\n",
    "        img_byte_arr2 = img_byte_arr2.getvalue()\n",
    "        \n",
    "        user_prompt = (\"Summarize what is visible in the current photo (the first one). \" + \n",
    "             \"How is it different from the previous photo (the second one)? There may be some subtle differences as well. \" + ending)\n",
    "\n",
    "    if local:\n",
    "    \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt,},\n",
    "            {\"role\": \"user\", \"content\": placeholder + user_prompt},\n",
    "        ]\n",
    "    \n",
    "        prompt = processor.tokenizer.apply_chat_template(\n",
    "          messages, \n",
    "          tokenize=False, \n",
    "          add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "        \n",
    "        generate_ids = model.generate(**inputs, \n",
    "          eos_token_id=processor.tokenizer.eos_token_id, \n",
    "          **generation_args\n",
    "        )\n",
    "        \n",
    "        # remove input tokens \n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        response = processor.batch_decode(generate_ids, \n",
    "          skip_special_tokens=True, \n",
    "          clean_up_tokenization_spaces=False)[0]\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logbox.append_stdout(\"Generating text finished in \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "        return response\n",
    "    else:\n",
    "        # Create the prompt with text and multiple images\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(system_instruction = system_prompt),\n",
    "            contents=[\n",
    "                user_prompt,\n",
    "                types.Part.from_bytes(\n",
    "                    data=img_byte_arr1,\n",
    "                    mime_type='image/png'\n",
    "                ),\n",
    "                types.Part.from_bytes(\n",
    "                    data=img_byte_arr2,\n",
    "                    mime_type='image/png'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logbox.append_stdout(\"Generating text finished in \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a498d1-5bde-4c5a-9dec-0ff7a2ddc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_screenshot():\n",
    "    windir = \"C:\\\\Users\\\\matis\\\\OneDrive\\\\Desktop\\\\script-it\\\\\"\n",
    "    lindir = \"/mnt/c/Users/matis/OneDrive/Desktop/script-it/\"\n",
    "\n",
    "    name = \"current_photo.png\"\n",
    "    if os.path.isfile(lindir+name):\n",
    "        shutil.copyfile(lindir+name, lindir+name.replace(\"current_photo\",\"previous_photo\"))\n",
    "    \n",
    "    subprocess.call(['/mnt/c/Users/matis/OneDrive/Desktop/script-it/nircmd.exe', 'cmdwait', '2000', 'savescreenshot', \n",
    "                     windir+'current_photo.png'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ee9302-d589-47aa-aafa-f8d9291490c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_gif(text):\n",
    "    text = text.lower()\n",
    "    outputs = []\n",
    "    if \"hello\" in text:\n",
    "        outputs.append(\"gifs/\"+\"waving\"+\"_bg.gif\")\n",
    "    if \"scar\" in text:\n",
    "        outputs.append(\"gifs/\"+\"scary\"+\"_bg.gif\")\n",
    "    if \"love\" in text:\n",
    "        outputs.append(\"gifs/\"+\"lovely\"+\"_bg.gif\")\n",
    "    if \"interest\" in text:\n",
    "        outputs.append(\"gifs/\"+\"lovely\"+\"_bg.gif\")\n",
    "    if \"happy\" in text:\n",
    "        outputs.append(\"gifs/\"+\"happy\"+\"_bg.gif\")\n",
    "\n",
    "    outputs.append(\"gifs/\"+\"talking\"+\"_bg.gif\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afcdf1f-fdba-47e0-a349-3bbda1c32497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(pipeline, text):\n",
    "    global countdown_state\n",
    "    start_time = time.time()\n",
    "    \n",
    "    imgfile1 = 'gifs/ctalking_bg.gif'\n",
    "    imgfile2 = 'gifs/xdancing_bg.gif'\n",
    "        \n",
    "    with output_image:\n",
    "        display(Image(filename=imgfile2))\n",
    "    \n",
    "    text = text.replace(\"first photo\", \"current photo\")\n",
    "    text = text.replace(\"second photo\", \"previous photo\")\n",
    "    \n",
    "    voice_tensor1 = torch.load('af_nicole.pt', weights_only=True)\n",
    "    voice_tensor2 = torch.load('jf_alpha.pt', weights_only=True)\n",
    "    t = 0.3\n",
    "    interp_voice = (1 - t) * voice_tensor1 + t * voice_tensor2\n",
    "\n",
    "    generator = pipeline(text, voice=interp_voice, speed=1, split_pattern=r'\\n+')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logbox.append_stdout(\"Generating speech finished in \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "    \n",
    "    for i, (gs, ps, audio) in enumerate(generator):\n",
    "        duration = math.ceil(librosa.get_duration(y=audio, sr=24000))\n",
    "        timeleft = int(duration) + 2\n",
    "        countdown_state[\"time_left\"] += timeleft\n",
    "        audio_data = Audio(data=audio, rate=24000, autoplay=True)\n",
    "        \n",
    "        # Remove the previously displayed audio and GIF\n",
    "        output_image.clear_output()\n",
    "        output_audio.clear_output()\n",
    "        textbox.clear_output()\n",
    "        textbox.outputs = []\n",
    "        \n",
    "        image_array = decide_gif(gs)\n",
    "        \n",
    "        with output_audio:\n",
    "            display(audio_data)\n",
    "            # display(Image(filename=imgfile1))\n",
    "\n",
    "        textbox.append_stdout(gs)\n",
    "        # with textbox:\n",
    "        #     print(gs)\n",
    "            \n",
    "        while timeleft > 0:\n",
    "            if len(image_array) > 0:\n",
    "                showing_image = image_array.pop(0)\n",
    "\n",
    "                output_image.clear_output()\n",
    "                with output_image:\n",
    "                    display(Image(filename=showing_image))\n",
    "                \n",
    "            if timeleft > 10:\n",
    "                time.sleep(10)\n",
    "                timeleft -= 10\n",
    "            else:\n",
    "                time.sleep(timeleft)\n",
    "                timeleft -= timeleft\n",
    "\n",
    "    # Revert back to the base image\n",
    "    output_image.clear_output()\n",
    "    output_audio.clear_output()\n",
    "    textbox.clear_output()\n",
    "    textbox.outputs = []\n",
    "    logbox.clear_output()\n",
    "    \n",
    "    with output_image:\n",
    "        display(Image(filename=imgfile2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7efa869-fd4f-474c-a441-ac7d93705c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208133647a304fc6a0342feaa6b0f212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Loop', style=ButtonStyle()), Button(description='Stop', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9fde2cde544966937c4e1b42edb34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='550px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517df84f2d3d4fa2b3d67bfdb878ec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='40px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee746d96e6744d9bc1dfeb8e63b33c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='100px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loop_flag = {\"running\": False}\n",
    "countdown_state = {\"time_left\": 21}\n",
    "\n",
    "button_start = widgets.Button(description=\"Loop\")\n",
    "button_stopp = widgets.Button(description=\"Stop\")\n",
    "output_image = widgets.Output(layout={'height': '550px'})\n",
    "output_audio = widgets.Output(layout={'height': '40px'})\n",
    "timer = widgets.Output()\n",
    "textbox = widgets.Output(layout={'height': '100px'})\n",
    "logbox = widgets.Output(layout={'height': '100px'})\n",
    "\n",
    "display(widgets.HBox((button_start, button_stopp, timer)), output_image, output_audio, textbox)\n",
    "with timer:\n",
    "    print(countdown_state[\"time_left\"])\n",
    "\n",
    "def runss_loop():\n",
    "    global loop_flag\n",
    "    global countdown_state\n",
    "          \n",
    "    filename1 = \"./current_photo.png\"\n",
    "    filename2 = \"./previous_photo.png\"\n",
    "    while loop_flag[\"running\"]:\n",
    "        countdown_state[\"time_left\"] = 20\n",
    "        timer.clear_output()\n",
    "        textbox.clear_output()\n",
    "        \n",
    "        take_screenshot()\n",
    "        if os.path.isfile(filename2):\n",
    "            text = generate_text(image_local, system_prompt, filename1, filename2, model, processor)\n",
    "        else:\n",
    "            text = generate_text(image_local, system_prompt, filename1, None, model, processor)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generate_audio(pipeline, text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # How much time past in the audio?\n",
    "        elapsed_time = end_time - start_time\n",
    "        if elapsed_time < 45.00:\n",
    "            countdown_state[\"time_left\"] = int(45 - elapsed_time)\n",
    "            logbox.append_stdout(\"Waiting \" + str(countdown_state[\"time_left\"]) + \" seconds...\")\n",
    "            time.sleep(countdown_state[\"time_left\"])\n",
    "                \n",
    "        if not loop_flag[\"running\"]:\n",
    "            break\n",
    "\n",
    "                \n",
    "def stops_loop(b):\n",
    "    global loop_flag\n",
    "    global countdown_state\n",
    "    \n",
    "    timer.clear_output()\n",
    "    textbox.clear_output()\n",
    "    \n",
    "    loop_flag[\"running\"] = False\n",
    "    countdown_state[\"time_left\"] = 0\n",
    "    \n",
    "    with textbox:\n",
    "        print(\"Game Over\")\n",
    "    \n",
    "    with timer:\n",
    "        print(countdown_state[\"time_left\"])\n",
    "\n",
    "def starts_loop():\n",
    "    global loop_flag\n",
    "    global countdown_state\n",
    "    \n",
    "    if not loop_flag[\"running\"]:\n",
    "        \n",
    "        loop_flag[\"running\"] = True\n",
    "        \n",
    "        thread = threading.Thread(target=runss_loop)\n",
    "        thread.start()\n",
    "        return \"Loop started.\"\n",
    "    return \"Loop already running.\"\n",
    "\n",
    "def update_timer(b):\n",
    "    global loop_flag\n",
    "    global countdown_state\n",
    "    \n",
    "    status = starts_loop()\n",
    "    with textbox:\n",
    "        print(status)\n",
    "    \n",
    "    while loop_flag[\"running\"]:\n",
    "        timer.clear_output()\n",
    "        with timer:\n",
    "            print(countdown_state[\"time_left\"])\n",
    "            countdown_state[\"time_left\"] -= 1\n",
    "            if not loop_flag[\"running\"]:\n",
    "                break\n",
    "        time.sleep(1)\n",
    "\n",
    "button_start.on_click(update_timer)\n",
    "button_stopp.on_click(stops_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510defa-5fbd-44b4-a7ad-c044088e43d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340efcd6-370d-4fca-9e3c-a3c299d101ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2182d-4fc4-4636-98df-12a0de560850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e3384-f9da-4cbe-af9e-ff5e47823ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91938dde-7cc5-46fe-9503-c8a0c439b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
